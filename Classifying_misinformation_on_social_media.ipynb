{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Classifying misinformation on social media - Baseline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Authors\n",
        "\n",
        "* Mihir Rao\n",
        "* Leah Chowenhill\n",
        "* Shubham Dubey\n",
        "* Aniket Sakpal"
      ],
      "metadata": {
        "id": "TkN14JdPjt8G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google Drive Mount"
      ],
      "metadata": {
        "id": "FE6u568AZVGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n"
      ],
      "metadata": {
        "id": "KHPG7v7kZY02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5702a72b-f9c6-4c33-b658-f0a8e6e93302"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries "
      ],
      "metadata": {
        "id": "M8V13KQ5Glq7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fKg8ozGSF0Mn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import torchaudio\n",
        "import torchaudio.functional as F\n",
        "import torchaudio.transforms as T\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.utils.data import random_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score, roc_auc_score, average_precision_score\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data From Drive"
      ],
      "metadata": {
        "id": "uZt4UYJ1UMeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip -q drive/MyDrive/upfd_data/gossipcop.zip -d gossipcop/\n",
        "# data_source = \"gossipcop\" \n",
        "\n",
        "!unzip -q drive/MyDrive/upfd_data/politifact.zip -d politifact/\n",
        "data_source = \"politifact\" "
      ],
      "metadata": {
        "id": "yDSOk3pdUkiL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_source = \"politifact\" "
      ],
      "metadata": {
        "id": "GFaYqSscTv8s"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the Node Level - Average Out Degree"
      ],
      "metadata": {
        "id": "CmNzGhPfGoJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Undersatnding-\n",
        "From pairs which are present in A, we are extracting the information of each pair and storing in processed edge list.\n",
        "In the end we are taking a count of all the mappings a node might have"
      ],
      "metadata": {
        "id": "66I3wbsFinMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = data_source + \"/\"\n",
        "\n",
        "with open(data_path + 'A.txt') as f:\n",
        "    edge_list = f.readlines()\n",
        "\n",
        "edge_list_processed = list()\n",
        "\n",
        "only_first_node = list()\n",
        "\n",
        "for i in range(len(edge_list)):\n",
        "    each_pair = edge_list[i].replace(\"\\n\", \"\").replace(\" \",\"\").split(\",\")\n",
        "\n",
        "    each_pair = [int(j) for j in each_pair]\n",
        "\n",
        "    edge_list_processed.append(each_pair)\n",
        "\n",
        "    only_first_node.append(each_pair[0])\n",
        "\n",
        "\n",
        "node_graph_mapping = np.load(data_path +'node_graph_id.npy')\n",
        "V = len(node_graph_mapping)\n",
        "edge_list_processed[0]\n",
        "from collections import Counter \n",
        "node_counter = Counter(only_first_node)\n",
        "len(node_counter.keys())\n",
        "for i in range(len(node_graph_mapping)):\n",
        "  if i not in node_counter.keys():\n",
        "    node_counter[i] = 0"
      ],
      "metadata": {
        "id": "5YaW8fHdGq8z"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = np.load(data_path +'graph_labels.npy')\n",
        "Y = torch.Tensor(Y).cuda()"
      ],
      "metadata": {
        "id": "uTypHmfjVXEm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lbl=np.load(data_path +'graph_labels.npy')"
      ],
      "metadata": {
        "id": "Cch-tyig7Oo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lbl.shape"
      ],
      "metadata": {
        "id": "XJPhz4i07TYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Node Attributes"
      ],
      "metadata": {
        "id": "NfGcSc1Txi3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding_method = \"spacy\"\n",
        "\n",
        "# embedding_method = \"content\"\n",
        "\n",
        "embedding_method = \"bert\""
      ],
      "metadata": {
        "id": "NjwBkaxRcOO-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# len(np.unique(node_graph_mapping))"
      ],
      "metadata": {
        "id": "mf8I9leLBae1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# len(node_graph_mapping)"
      ],
      "metadata": {
        "id": "zm_g_NNECUue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# len(edge_list_processed)"
      ],
      "metadata": {
        "id": "leVQ1Y3-DuMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_node_id = np.cumsum(np.bincount(node_graph_mapping))\n",
        "\n",
        "source_node_id = np.insert(source_node_id, 0, 0)\n",
        "source_node_id=source_node_id[:len(source_node_id) - 1]"
      ],
      "metadata": {
        "id": "21yesviDzq48"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# source_node_id_test = np.append(0,source_node_id)"
      ],
      "metadata": {
        "id": "NRsxtvVE5fZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# len(source_node_id)"
      ],
      "metadata": {
        "id": "FkvozdHEFlMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# source_node_id"
      ],
      "metadata": {
        "id": "IPjVcq-mFp5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Undersatnding-\n",
        "re-shaping the data based on the embedding method"
      ],
      "metadata": {
        "id": "q56YB_2_rJEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding_file_path = data_path +\"new_\" + embedding_method + \"_feature.npz\"\n",
        "\n",
        "# new_spacy_feature = np.load(embedding_file_path)\n",
        "# new_spacy_feature.files\n",
        "# spacy_data = new_spacy_feature['data']\n",
        "\n",
        "# if embedding_method == \"spacy\":\n",
        "#     spacy_data = spacy_data.reshape(len(node_graph_mapping), 300)\n",
        "# elif  embedding_method == \"bert\": \n",
        "#     spacy_data = spacy_data.reshape(len(node_graph_mapping), 768)\n",
        "# elif embedding_method == \"content\": \n",
        "#     spacy_data = spacy_data.reshape(len(node_graph_mapping), 310)\n",
        "    \n",
        "# spacy_data.shape"
      ],
      "metadata": {
        "id": "oEo9I7xb6KgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# spacy_data[0,:20]"
      ],
      "metadata": {
        "id": "PoW9jfWA6L06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# spacy_data[1,:20]"
      ],
      "metadata": {
        "id": "l6XiybIfOfa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding_file_path = data_path +\"new_\" + embedding_method + \"_feature.npz\"\n",
        "\n",
        "# new_spacy_feature = np.load(embedding_file_path)\n",
        "# new_spacy_feature.files\n",
        "# spacy_data = new_spacy_feature['data']\n",
        "\n",
        "# if embedding_method == \"spacy\":\n",
        "#     spacy_data = spacy_data.reshape(len(node_graph_mapping), 300)\n",
        "# elif  embedding_method == \"bert\": \n",
        "#     spacy_data = spacy_data.reshape(len(node_graph_mapping), 768)\n",
        "# elif embedding_method == \"content\": \n",
        "#     spacy_data = spacy_data.reshape(len(node_graph_mapping), 310)\n",
        "    \n",
        "# spacy_data.shape\n",
        "# # out_degree_nodes  = np.array(list(node_counter.values()))\n",
        "# # out_degree_nodes.shape\n",
        "# # out_degree_nodes = out_degree_nodes.reshape(out_degree_nodes.shape[0],1)\n",
        "# # proc_data = np.append(spacy_data,out_degree_nodes, 1)\n",
        "# # proc_data.shape\n",
        "# node_slice = torch.from_numpy(np.bincount(node_graph_mapping))\n",
        "\n",
        "# node_slice=node_slice.cuda()\n",
        "# #node_slice = np.array(node_slice)\n",
        "\n",
        "# proc_data = torch.tensor(spacy_data)\n",
        "\n",
        "# proc_data = proc_data.cuda()\n",
        "\n",
        "# node_slice = tuple(node_slice)\n",
        "# # all_graphs_data = list()\n",
        "# # for i in node_slice:\n",
        "# #     # graph_level_data = proc_data[0:i,:]\n",
        "# #     all_graphs_data.append(list(proc_data[0:i,:].flatten()))\n",
        "\n",
        "# graph_split_data = list(torch.split(proc_data, node_slice))\n",
        "# all_graphs_data = list()"
      ],
      "metadata": {
        "id": "GddCKbUi631j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# graph_split_data[0].shape"
      ],
      "metadata": {
        "id": "3JXDSoTdR2oG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x_testing=torch.mean(graph_split_data[0],0)"
      ],
      "metadata": {
        "id": "Ipy2YLzASFCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x_testing.shape"
      ],
      "metadata": {
        "id": "3fMQRVb6SyRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_file_path = data_path +\"new_\" + embedding_method + \"_feature.npz\"\n",
        "\n",
        "new_spacy_feature = np.load(embedding_file_path)\n",
        "new_spacy_feature.files\n",
        "spacy_data = new_spacy_feature['data']\n",
        "\n",
        "if embedding_method == \"spacy\":\n",
        "    spacy_data = spacy_data.reshape(len(node_graph_mapping), 300)\n",
        "elif  embedding_method == \"bert\": \n",
        "    spacy_data = spacy_data.reshape(len(node_graph_mapping), 768)\n",
        "elif embedding_method == \"content\": \n",
        "    spacy_data = spacy_data.reshape(len(node_graph_mapping), 310)\n",
        "    \n",
        "spacy_data.shape\n",
        "# out_degree_nodes  = np.array(list(node_counter.values()))\n",
        "# out_degree_nodes.shape\n",
        "# out_degree_nodes = out_degree_nodes.reshape(out_degree_nodes.shape[0],1)\n",
        "# proc_data = np.append(spacy_data,out_degree_nodes, 1)\n",
        "# proc_data.shape\n",
        "node_slice = torch.from_numpy(np.bincount(node_graph_mapping))\n",
        "\n",
        "node_slice=node_slice.cuda()\n",
        "#node_slice = np.array(node_slice)\n",
        "\n",
        "proc_data = torch.tensor(spacy_data)\n",
        "\n",
        "proc_data = proc_data.cuda()\n",
        "\n",
        "node_slice = tuple(node_slice)\n",
        "# all_graphs_data = list()\n",
        "# for i in node_slice:\n",
        "#     # graph_level_data = proc_data[0:i,:]\n",
        "#     all_graphs_data.append(list(proc_data[0:i,:].flatten()))\n",
        "\n",
        "graph_split_data = list(torch.split(proc_data, node_slice))\n",
        "all_graphs_data = list()\n",
        "for each_graph in graph_split_data:\n",
        "  # print(graph_split_data)\n",
        "  a=torch.mean(each_graph[1:],0)\n",
        "  b=each_graph[0]\n",
        "  c=torch.cat((a,b),dim=0)\n",
        "  all_graphs_data.append(c)\n",
        "    \n",
        "all_graphs_data_padded = pad_sequence(all_graphs_data, batch_first= True) \n",
        "X = all_graphs_data_padded"
      ],
      "metadata": {
        "id": "osgQdLN3IFB3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0zbpQzrHC9B",
        "outputId": "04be5230-1152-4737-aebe-6b6c049405fc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([314, 1536])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Check\n",
        "check = list()\n",
        "for i in all_graphs_data_padded:\n",
        "  check.append(len(i))\n",
        "\n",
        "if embedding_method == \"spacy\":\n",
        "    list(set(check))[0]/301\n",
        "else:\n",
        "    list(set(check))[0]/769\n",
        "  \n",
        "len(all_graphs_data_padded[0])\n",
        "all_graphs_data_padded[1]"
      ],
      "metadata": {
        "id": "6yrjYynxDElT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "762741ff-13c0-42e1-95f4-1ff4d76ed60e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0244,  0.0328,  0.0164,  ..., -0.0707, -0.0849,  0.5245],\n",
              "       device='cuda:0', dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_graphs_data_padded.shape\n",
        "\n",
        "\n",
        "train_len=int(0.7*len(X))\n",
        "val_len=int(0.2*len(X))\n",
        "test_len=len(X)-val_len-train_len\n",
        "train_len,val_len,test_len\n",
        "mn=np.split(np.arange(5464),(train_len,val_len,test_len))\n",
        "mn_x=torch.split(X,(train_len,val_len,test_len))\n",
        "mn_y=torch.split(Y,(train_len,val_len,test_len))\n",
        "\n",
        "for x in mn_x:\n",
        "  print(x.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEzrrUauWtTy",
        "outputId": "ca3ca027-3973-4b70-991e-efed3aa22ded"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([219, 1536])\n",
            "torch.Size([62, 1536])\n",
            "torch.Size([33, 1536])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split into Train, Validation, Test"
      ],
      "metadata": {
        "id": "_ucpmCijFcei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "np.random.seed(17)\n",
        "a=np.arange(X.shape[0])\n",
        "np.random.shuffle(a)\n",
        "\n",
        "\n",
        "\n",
        "train_len=int(0.7*len(X))\n",
        "val_len=int(0.2*len(X))\n",
        "test_len=len(X)-val_len-train_len\n",
        "train_len,val_len,test_len\n",
        "#mn=np.split(np.random.shuffle(np.arange(5464)),(train_len,val_len,test_len))\n",
        "X=X[a]\n",
        "Y=Y[a]\n",
        "mn_x=torch.split(X,(train_len,val_len,test_len))\n",
        "mn_y=torch.split(Y,(train_len,val_len,test_len))\n",
        "\n",
        "for x in mn_x:\n",
        "  print(x.shape)"
      ],
      "metadata": {
        "id": "qVMK4Pq0ZGkf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ab89120-3541-4c10-a43c-40f45525ebd4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([219, 1536])\n",
            "torch.Size([62, 1536])\n",
            "torch.Size([33, 1536])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_training = int(len(X) * 0.2)\n",
        "num_val = int(len(X) * 0.1)\n",
        "num_test = len(X) - (num_training + num_val)\n",
        "\n",
        "#x_train, x_val, y_train, y_val = train_test_split( X, Y, test_size = 0.3 ,random_state=17 )\n",
        "#x_train, x_val, y_train, y_val =X[mn[0]],X[mn[1]],Y[mn[0]],Y[mn[1]]\n",
        "\n",
        "x_train, x_val, x_test=mn_x[0],mn_x[1], mn_x[2]\n",
        "\n",
        "y_train, y_val, y_test=mn_y[0],mn_y[1], mn_y[2]\n",
        "\n",
        "x_test=mn_x[2]\n",
        "\n",
        "\n",
        "#x_val , x_test , y_val , y_test = train_test_split(x_val, y_val, test_size = 0.33, random_state=17)\n",
        "\n",
        "#x_test,y_test=X[mn[2]],Y[mn[2]]"
      ],
      "metadata": {
        "id": "ZpYg0__MuR_-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape,x_val.shape,x_test.shape)\n",
        "y_train.shape,y_val.shape,y_test.shape\n",
        "\n"
      ],
      "metadata": {
        "id": "Dqy5oOPl-qWI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc1f9f66-1704-4083-907c-7fb22877e63f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([219, 1536]) torch.Size([62, 1536]) torch.Size([33, 1536])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([219]), torch.Size([62]), torch.Size([33]))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# y_train[:100]"
      ],
      "metadata": {
        "id": "m0hOD51GuBba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataloader "
      ],
      "metadata": {
        "id": "LuWVGNDhUWNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Fetch_Rows(torch.utils.data.Dataset):\n",
        "  def __init__(self, X, Y = None, only_test = False):\n",
        "    self.only_test = only_test\n",
        "\n",
        "    self.X = X\n",
        "    \n",
        "    if only_test==False:\n",
        "       self.Y = Y\n",
        "    \n",
        "    self.length = len(X)\n",
        "\n",
        " \n",
        "  def __len__(self):\n",
        "      return self.length\n",
        "        \n",
        "  def __getitem__(self, i):\n",
        "\n",
        "      xx = self.X[i]\n",
        "      \n",
        "      if self.only_test == False:\n",
        "          yy = self.Y[i]   \n",
        "\n",
        "      if self.only_test == False:\n",
        "        return xx, yy\n",
        "      else:\n",
        "        return xx\n",
        "    "
      ],
      "metadata": {
        "id": "nqUHNyXzKgzA"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network"
      ],
      "metadata": {
        "id": "kbrdhM3oGiAC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4 layered MLP - inverted cone"
      ],
      "metadata": {
        "id": "1FVDqp1XTRyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(torch.nn.Module):\n",
        "    def __init__(self, in_size):\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        layers = [\n",
        "            nn.Linear(in_size,100),\n",
        "            nn.Linear(100, 50),\n",
        "            nn.Linear(50,25),\n",
        "            nn.Linear(25,2)\n",
        "        ]\n",
        "\n",
        "        self.laysers = nn.Sequential(*layers)\n",
        "       \n",
        "\n",
        "    def forward(self, A0):\n",
        "        x = self.laysers(A0)\n",
        "\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "PUOdnvABXoTT"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train & Validation & Test Configuration"
      ],
      "metadata": {
        "id": "GFEKjqoNGlcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def train(model, train_loader, optimizer, criterion):\n",
        "#     model.train()\n",
        "    \n",
        "#     total_loss = 0\n",
        "\n",
        "#     for i, data in (enumerate(train_loader)):\n",
        "        \n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         x, y = data\n",
        "#         x = x.float().cuda()\n",
        "#         y = y.long().cuda()\n",
        "\n",
        "#         output= model(x)\n",
        "\n",
        "#         # print(x)\n",
        "#         # print(type(x))\n",
        "#         # print(type(output))\n",
        "#         # print(output)\n",
        "#         # print(y)\n",
        "\n",
        "#         loss = criterion(output, y)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         total_loss += loss.item()\n",
        "      \n",
        "#     avg_loss = total_loss/(i+1)\n",
        "\n",
        "#     return model, avg_loss\n",
        "\n",
        "def train(model, train_loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    \n",
        "    total_loss = 0\n",
        "\n",
        "    for i, data in (enumerate(train_loader)):\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x, y = data\n",
        "        x = x.float().cuda()\n",
        "        y = y.long().cuda()\n",
        "\n",
        "        output= model(x)\n",
        "\n",
        "        # print(x)\n",
        "        # print(type(x))\n",
        "        # print(type(output))\n",
        "        # print(output)\n",
        "        # print(y)\n",
        "\n",
        "        loss = criterion(output, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    return model,loss.item()\n",
        "\n",
        "\n",
        "\n",
        "def validation( model, device, val_loader):\n",
        "    model.eval()\n",
        "    true_y_list = []\n",
        "    pred_y_list = []\n",
        "    with torch.no_grad():\n",
        "      for data, true_y in val_loader:\n",
        "          data = data.float().to(device)\n",
        "          true_y = true_y.long().to(device)                \n",
        "          \n",
        "          output = model(data)\n",
        "\n",
        "          pred_y = torch.argmax(output, axis=1)\n",
        "\n",
        "          pred_y_list.extend(pred_y.tolist())\n",
        "          \n",
        "          true_y_list.extend(true_y.tolist())\n",
        "\n",
        "    val_accuracy =  accuracy_score(true_y_list, pred_y_list)\n",
        "    accuracy = accuracy_score(true_y_list, pred_y_list)\n",
        "    f1_macro = f1_score(true_y_list, pred_y_list, average='macro') \n",
        "    f1_micro = f1_score(true_y_list, pred_y_list, average='micro') \n",
        "    precision = precision_score(true_y_list, pred_y_list, zero_division=0)\n",
        "    recall = recall_score(true_y_list, pred_y_list, zero_division=0)\n",
        "\n",
        "    return val_accuracy, f1_macro, f1_micro, precision, recall    "
      ],
      "metadata": {
        "id": "KQ83QTbmXq6Y"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def validation( model, device, val_loader):\n",
        "#     model.eval()\n",
        "#     true_y_list = []\n",
        "#     pred_y_list = []\n",
        "#     with torch.no_grad():\n",
        "#       for data, true_y in val_loader:\n",
        "#           data = data.float().to(device)\n",
        "#           true_y = true_y.long().to(device)                \n",
        "          \n",
        "#           output = model(data)\n",
        "\n",
        "#           pred_y = torch.argmax(output, axis=1)\n",
        "\n",
        "#           pred_y_list.extend(pred_y.tolist())\n",
        "          \n",
        "#           true_y_list.extend(true_y.tolist())\n",
        "\n",
        "#     val_accuracy =  accuracy_score(true_y_list, pred_y_list)\n",
        "#     accuracy = accuracy_score(true_y_list, pred_y_list)\n",
        "#     f1_macro = f1_score(true_y_list, pred_y_list, average='macro') \n",
        "#     f1_micro = f1_score(true_y_list, pred_y_list, average='micro') \n",
        "#     precision = precision_score(true_y_list, pred_y_list, zero_division=0)\n",
        "#     recall = recall_score(true_y_list, pred_y_list, zero_division=0)\n",
        "\n",
        "#     return val_accuracy, f1_macro, f1_micro, precision, recall"
      ],
      "metadata": {
        "id": "NRX3aelBcPft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test( model, device, test_loader,y_test=None,my_dataset=True,):\n",
        "    model.eval()\n",
        "    y_test1 = []\n",
        "    pred_y_list = []\n",
        "    with torch.no_grad():\n",
        "      for data_x in test_loader:\n",
        "          \n",
        "          data_x = data_x.float().to(device)               \n",
        "          \n",
        "          output = model(data_x)\n",
        "\n",
        "          pred_y = torch.argmax(output, axis=1)\n",
        "\n",
        "          pred_y_list.extend(pred_y.tolist())\n",
        "          \n",
        "    if my_dataset:\n",
        "      y_test = y_test.long().to(device) \n",
        "      y_test1.extend(y_test.tolist())\n",
        "      val_accuracy =  accuracy_score(y_test1, pred_y_list)\n",
        "      accuracy = accuracy_score(y_test1, pred_y_list)\n",
        "      f1_macro = f1_score(y_test1, pred_y_list, average='macro') \n",
        "      f1_micro = f1_score(y_test1, pred_y_list, average='micro') \n",
        "      precision = precision_score(y_test1, pred_y_list, zero_division=0)\n",
        "      recall = recall_score(y_test1, pred_y_list, zero_division=0)\n",
        "      # print(confusion_matrix(y_test1, pred_y_list))\n",
        "      # round(answer, 2)\n",
        "\n",
        "    return round(val_accuracy,2), round(f1_macro,2), round(f1_micro,2), round(precision,2), round(recall,2)"
      ],
      "metadata": {
        "id": "zicV8jYMR7Fu"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training "
      ],
      "metadata": {
        "id": "xW9ODuy1G0-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Network(in_size = len(all_graphs_data_padded[0])).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "scheduler=StepLR(optimizer,step_size=1,gamma=0.2)"
      ],
      "metadata": {
        "id": "kejk6f4fXvFi"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_items = Fetch_Rows(x_train, y_train)\n",
        "train_loader = torch.utils.data.DataLoader(train_items,shuffle=True  )\n",
        "\n",
        "val_items = Fetch_Rows(x_val, y_val)\n",
        "val_loader = torch.utils.data.DataLoader(val_items,shuffle=False)\n",
        "\n",
        "test_items = Fetch_Rows(x_test, y_test,only_test=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_items, shuffle=False)\n",
        "\n",
        "\n",
        "for epoch in range(1, 30):\n",
        "    model ,avg_loss = train( model,  train_loader, optimizer, criterion)\n",
        "    \n",
        "    val_accuracy, f1_macro, f1_micro, precision, recall = validation(model, device, val_loader)\n",
        "\n",
        "    print('Epoch',epoch,\" Training Loss \",avg_loss , \"Validation Accuracy \", val_accuracy, \"F1 Macro \", f1_macro, \"F1 Micro \",f1_micro, \"Precision \", precision, \"Recall \", recall)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ovsxE-1YS6m",
        "outputId": "afdbbadb-dbc2-443d-a803-53affdd653c2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1  Training Loss  0.21678507328033447 Validation Accuracy  0.5 F1 Macro  0.4202111613876319 F1 Micro  0.5 Precision  0.46551724137931033 Recall  1.0\n",
            "Epoch 2  Training Loss  0.13608413934707642 Validation Accuracy  0.6935483870967742 F1 Macro  0.6895915678524375 F1 Micro  0.6935483870967742 Precision  0.5952380952380952 Recall  0.9259259259259259\n",
            "Epoch 3  Training Loss  0.8194882273674011 Validation Accuracy  0.5645161290322581 F1 Macro  0.5193798449612402 F1 Micro  0.5645161290322581 Precision  0.5 Recall  1.0\n",
            "Epoch 4  Training Loss  0.28896364569664 Validation Accuracy  0.7741935483870968 F1 Macro  0.7732497387669801 F1 Micro  0.7741935483870968 Precision  0.6666666666666666 Recall  0.9629629629629629\n",
            "Epoch 5  Training Loss  6.19869097135961e-05 Validation Accuracy  0.8064516129032258 F1 Macro  0.7926421404682275 F1 Micro  0.8064516129032258 Precision  0.8947368421052632 Recall  0.6296296296296297\n",
            "Epoch 6  Training Loss  0.28098469972610474 Validation Accuracy  0.8225806451612904 F1 Macro  0.8225344782721833 F1 Micro  0.8225806451612904 Precision  0.7352941176470589 Recall  0.9259259259259259\n",
            "Epoch 7  Training Loss  7.545663538621739e-05 Validation Accuracy  0.7419354838709677 F1 Macro  0.6964504283965728 F1 Micro  0.7419354838709677 Precision  1.0 Recall  0.4074074074074074\n",
            "Epoch 8  Training Loss  0.0007023728103376925 Validation Accuracy  0.8387096774193549 F1 Macro  0.8238636363636365 F1 Micro  0.8387096774193549 Precision  1.0 Recall  0.6296296296296297\n",
            "Epoch 9  Training Loss  0.08529956638813019 Validation Accuracy  0.8225806451612904 F1 Macro  0.8187616263619453 F1 Micro  0.8225806451612904 Precision  0.8076923076923077 Recall  0.7777777777777778\n",
            "Epoch 10  Training Loss  0.0034023988991975784 Validation Accuracy  0.8225806451612904 F1 Macro  0.8081575246132209 F1 Micro  0.8225806451612904 Precision  0.9444444444444444 Recall  0.6296296296296297\n",
            "Epoch 11  Training Loss  4.172316494077677e-06 Validation Accuracy  0.8225806451612904 F1 Macro  0.8225344782721833 F1 Micro  0.8225806451612904 Precision  0.7352941176470589 Recall  0.9259259259259259\n",
            "Epoch 12  Training Loss  0.05712885782122612 Validation Accuracy  0.8225806451612904 F1 Macro  0.8115501519756838 F1 Micro  0.8225806451612904 Precision  0.9 Recall  0.6666666666666666\n",
            "Epoch 13  Training Loss  0.7840394973754883 Validation Accuracy  0.7741935483870968 F1 Macro  0.7654054054054054 F1 Micro  0.7741935483870968 Precision  0.782608695652174 Recall  0.6666666666666666\n",
            "Epoch 14  Training Loss  1.156323378381785e-05 Validation Accuracy  0.7903225806451613 F1 Macro  0.7902680197762165 F1 Micro  0.7903225806451614 Precision  0.7058823529411765 Recall  0.8888888888888888\n",
            "Epoch 15  Training Loss  0.610683798789978 Validation Accuracy  0.8064516129032258 F1 Macro  0.8062499999999999 F1 Micro  0.8064516129032258 Precision  0.7272727272727273 Recall  0.8888888888888888\n",
            "Epoch 16  Training Loss  5.6622808187967166e-05 Validation Accuracy  0.8387096774193549 F1 Macro  0.8385416666666665 F1 Micro  0.8387096774193549 Precision  0.7297297297297297 Recall  1.0\n",
            "Epoch 17  Training Loss  0.1814688891172409 Validation Accuracy  0.8225806451612904 F1 Macro  0.8225344782721833 F1 Micro  0.8225806451612904 Precision  0.7352941176470589 Recall  0.9259259259259259\n",
            "Epoch 18  Training Loss  6.5205356804654e-05 Validation Accuracy  0.7903225806451613 F1 Macro  0.7835079237174323 F1 Micro  0.7903225806451614 Precision  0.7916666666666666 Recall  0.7037037037037037\n",
            "Epoch 19  Training Loss  0.010661311447620392 Validation Accuracy  0.8064516129032258 F1 Macro  0.8031746031746032 F1 Micro  0.8064516129032258 Precision  0.7777777777777778 Recall  0.7777777777777778\n",
            "Epoch 20  Training Loss  0.0004898302140645683 Validation Accuracy  0.8225806451612904 F1 Macro  0.8214192196910186 F1 Micro  0.8225806451612904 Precision  0.7666666666666667 Recall  0.8518518518518519\n",
            "Epoch 21  Training Loss  0.001625646254979074 Validation Accuracy  0.7903225806451613 F1 Macro  0.7876152832674572 F1 Micro  0.7903225806451614 Precision  0.75 Recall  0.7777777777777778\n",
            "Epoch 22  Training Loss  0.0 Validation Accuracy  0.8225806451612904 F1 Macro  0.8221642764015644 F1 Micro  0.8225806451612904 Precision  0.75 Recall  0.8888888888888888\n",
            "Epoch 23  Training Loss  2.861018856492592e-06 Validation Accuracy  0.8064516129032258 F1 Macro  0.8062499999999999 F1 Micro  0.8064516129032258 Precision  0.7272727272727273 Recall  0.8888888888888888\n",
            "Epoch 24  Training Loss  4.768370445162873e-07 Validation Accuracy  0.8225806451612904 F1 Macro  0.8214192196910186 F1 Micro  0.8225806451612904 Precision  0.7666666666666667 Recall  0.8518518518518519\n",
            "Epoch 25  Training Loss  9.536697689327411e-06 Validation Accuracy  0.8225806451612904 F1 Macro  0.8221642764015644 F1 Micro  0.8225806451612904 Precision  0.75 Recall  0.8888888888888888\n",
            "Epoch 26  Training Loss  1.4066597032069694e-05 Validation Accuracy  0.8225806451612904 F1 Macro  0.8221642764015644 F1 Micro  0.8225806451612904 Precision  0.75 Recall  0.8888888888888888\n",
            "Epoch 27  Training Loss  2.50339189733495e-06 Validation Accuracy  0.8225806451612904 F1 Macro  0.8221642764015644 F1 Micro  0.8225806451612904 Precision  0.75 Recall  0.8888888888888888\n",
            "Epoch 28  Training Loss  0.0 Validation Accuracy  0.8064516129032258 F1 Macro  0.8056426332288402 F1 Micro  0.8064516129032258 Precision  0.7419354838709677 Recall  0.8518518518518519\n",
            "Epoch 29  Training Loss  0.0 Validation Accuracy  0.8225806451612904 F1 Macro  0.8221642764015644 F1 Micro  0.8225806451612904 Precision  0.75 Recall  0.8888888888888888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "uxm-CdSFSJh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_accuracy, f1_macro, f1_micro, precision, recall=test(model,device,test_loader,y_test,my_dataset=True)\n",
        "print(\"Test Accuracy \", val_accuracy, \"\\nF1 Macro \", f1_macro, \"\\nF1 Micro \",f1_micro, \"\\nPrecision \", precision, \"\\nRecall \", recall)\n"
      ],
      "metadata": {
        "id": "AlMLLu5zVccy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c752903-0bb2-4e94-e23b-e43b1eb5b1e7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy  0.97 \n",
            "F1 Macro  0.97 \n",
            "F1 Micro  0.97 \n",
            "Precision  1.0 \n",
            "Recall  0.94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "i9J6kXjOkQvi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}